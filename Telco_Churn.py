# -*- coding: utf-8 -*-
"""PROJECT-KEL5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tVWb2B1oGbQ4EaFx57PUpzPTXvuTrpAR

# Understand & Identify Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# Import library
import numpy as np
import pandas as pd
import seaborn as sns
from collections import Counter

# ML library
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

# Pre-process
from sklearn import svm, preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import OneHotEncoder

# Imbalance
from imblearn.over_sampling import SMOTE
from sklearn.utils import resample

# Feature selection
from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel

# Evaluation metrics
from sklearn import metrics
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score

# Hyperparameter Tunning (GridSearchCV)
from sklearn.model_selection import GridSearchCV

# Visualisation
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import matplotlib
# %matplotlib inline
import os

import warnings
warnings.filterwarnings('ignore')

"""**Dataset Telco Customer Churn**"""

data = pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
data.head()

# change the type of position variable into float
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')

"""DESCRIPTION DATASET

**Classification labels**

Churn — Apakah kostumer churn atau tidak (Yes or No)


**Customer services booked**

PhoneService — Apakah kostumer memilik phone service (Yes, No)

MultipleLines — Apakah kostumer memiliki multiple lines (Yes, No, No phone service)

InternetService — Customer’s internet service provider (DSL, Fiber optic, No)

OnlineSecurity — Apakah kostumer memiliki online security (Yes, No, No internet service)

OnlineBackup — Apakah kostumer memiliki online backup (Yes, No, No internet service)

DeviceProtection — Apakah kostumer memiliki device protection (Yes, No, No internet service)

TechSupport — Apakah kostumer memiliki tech support (Yes, No, No internet service)

StreamingTV — Apakah kostumer menggunakan streaming TV (Yes, No, No internet service)

StreamingMovies — Apakah kostumer menggunakan streaming movies (Yes, No, No internet service)


**Customer account information**

Tenure — Jumlah bulan kostumer berada di perusahaan

Contract — Term kontrak kostumer (Month-to-month, One year, Two year)

PaperlessBilling — Apakah kostumer menggunakan paperless billing (Yes, No)

PaymentMethod — Metode pembayaran kostumer (Electronic check, 

Mailed check, Bank transfer (automatic), Credit card (automatic))

MonthlyCharges — Biaya yang dibebankan ke kostumer setiap bulan

TotalCharges — Total biaya yang dibebankan ke kostumer

**Customers demographic info**

customerID — ID kostumer

Gender — Apakah gender kostumer pria atau wanita

SeniorCitizen — Apakah kostumer warga yang sudah berumuru atau tidak (1, 0)

Partner — Apakah kostumer memiliki pasangan atau tidak (Yes, No)

Dependents — Apakah kostumer miliki tanggungan atau tidak (Yes, No)
"""

#mendeskripsikan data yang ada pada dataset 
print(data.info())
#mencari nilai statistik penyebaran nya
print(data.describe())
print(data.shape)
print(data.isnull().sum())

"""*   Terdapat **3** variabel numeric yaitu : tenure, MonthlyCharges dan TotalCharges

*   Terdapat **17** variabel categorical yaitu : customerID,	gender,	SeniorCitizen, Partner, Dependents,	PhoneService,	MultipleLines,	InternetService,	OnlineSecurity,	OnlineBackup,	DeviceProtection,	TechSupport,	StreamingTV,	StreamingMovies,	Contract,	PaperlessBilling,	PaymentMethod dan	Churn

*   Variabel **Churn** menjadi target variabel.

*   Terdapat Null Value pada TotalCharges



"""

# mencari missing data pada dataset
total_missing = data.isnull().sum().sort_values(ascending=False)
percent_1 = data.isnull().sum()/data.isnull().count()*100
percent_2 = (round(percent_1, 1)).sort_values(ascending=False)
missing_data = pd.concat([total_missing, percent_2], axis=1, keys=['Total Missing', '%'])
missing_data

for i in data.columns:
    if data[i].dtypes=="object":
        print(f'{i} : {data[i].unique()}')

"""#EDA

###Target Variabel
"""

labels = data['Churn'].unique()
values = data['Churn'].value_counts()
colours = ['rgb(205, 152, 36)',  'rgb(18, 36, 37)']
diag = go.Figure(data=[go.Pie(labels=labels, values=values,textinfo='label+text+value+percent',hole=.3, marker_colors= colours ,pull=[0.05, 0.05, 0.05])])
diag.show()

"""###Correlation with data Numeric"""

#Correlation between data
data.corr()

sns.heatmap(data.corr(), annot=True,cmap='cividis',)

"""Angka 0 tidak memiliki hubungan sedangkan angka yang mendekati 1 memiliki korelasi yang kuat(positif) dan mendekati -1 memiliki korelasi yang kuat(negatif).

* TotalCharges dan tenure memiliki korelasi yang kuat (positif)


"""

#Visualisasi korelasi variabel diatas menggunakan scatterplot
num_var = ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']

plt.figure(figsize=(12, 7),)
sns.pairplot(data, hue='Churn', height=2.5)
plt.show()

fig, (ax1, ax2, ax3) = plt.subplots(3)
sns.kdeplot(data["tenure"], shade=True, color="y",ax = ax1)
sns.kdeplot(data["MonthlyCharges"], shade=True, color="r", ax = ax2)
sns.kdeplot(data["TotalCharges"], shade=True, color="b", ax = ax3)
fig.tight_layout()
plt.show(fig)

mean = data['MonthlyCharges'].mean()
median = data['MonthlyCharges'].median()
print("Rata-Rata Monthly Charges {:2f}".format(round(mean,2)))
print("Median Monthly Charges {}".format(int(median)))

mean = data['tenure'].mean()
median = data['tenure'].median()
print("Rata-Rata tenure {:2f}".format(round(mean,2)))
print("Median tenure {}".format(int(median)))

mean = data['TotalCharges'].mean()
median = data['TotalCharges'].median()
print("Rata-Rata TotalCharges {:2f}".format(round(mean,2)))
print("Median TotalCharges {}".format(int(median)))

"""###BOXPLOT"""

sns.boxplot(data=data, x='Churn', y='TotalCharges')
plt.show()

sns.boxplot(data=data, x='Churn', y='tenure')
plt.show()

sns.boxplot(data=data, x='Churn', y='MonthlyCharges')
plt.show()

"""### Total distribution churn with data object"""

def category_rel_y(data):
    X=data.columns
    #print(df[X[1]])
 
    #print(len(X))
 
    for i in range(1,len(X)-2):
        
    
        if (data[X[i]].dtype=='object'):
                fig, axs = plt.subplots(1, 2,figsize=(13,3))
                tab_values=pd.crosstab(data[X[i]],data.iloc[:,20])
                #Creating  % of total values cross tab by using 'normalize=True' in normal tab
                tab_percentage=pd.crosstab(data[X[i]],data.iloc[:,20],normalize=True)
                #print(tab_percentage)
                tab_values.plot(kind='bar', stacked=False ,ax=axs[0],color=['black', 'yellow'])
                sns.heatmap(tab_percentage,annot=True,cmap='cividis',ax=axs[1],)
                plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=1, hspace=None)
                #sns.barplot(tab)
                axs[0].set_title('Actual counts for CHURN output versus '+X[i].upper(),fontweight='semibold', style = 'italic', fontsize=10, color='black')
              
                plt.title("% of total distribution for CHURN and "+(X[i]).upper(), fontweight='semibold', style = 'italic', fontsize=10, color='black')
                 
                plt.show()
                 
            
        
        
 #plt.show()     
category_rel_y(data)

"""**Churn dan MultipleLines**"""

data['MultipleLines'].unique()

sns.histplot(data=data, x='MultipleLines', hue='Churn', stat='probability', fill=True)
plt.show()

fig = px.histogram(data, x="MultipleLines", color="Churn",width=400, height=400)
fig.show()

"""**Churn dan InternetService**"""

data['InternetService'].unique()

sns.histplot(data=data, x='InternetService', hue='Churn', stat='probability', fill=True)
plt.show()

fig = px.histogram(data, x="InternetService", color="Churn",width=400, height=400)
fig.show()

"""**Churn dan OnlineSecurity**"""

data['OnlineSecurity'].unique()

sns.histplot(data=data, x='OnlineSecurity', hue='Churn', stat='probability', fill=True)
plt.show()

fig = px.histogram(data, x="OnlineSecurity", color="Churn",width=400, height=400)
fig.show()

"""#Pre - Processing


"""

data = data.dropna()
data.isnull().sum()

data = data.drop(['customerID'], axis = 1)

for i in data.columns:
    if data[i].dtypes=="object":
        print(f'{i} : {data[i].unique()}')

"""Replace Data Target"""

data['Churn'].replace(to_replace='Yes', value=1, inplace=True)
data['Churn'].replace(to_replace='No',  value=0, inplace=True)

data.head()

"""**One-Hot Encoding**"""

#One-Hot encoder untuk data gender
enc_oht_gender = OneHotEncoder(categories=[['Female','Male']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['gender']).reshape(-1,1)
enc_oht_gender.fit(y)
y1 =  enc_oht_gender.transform(y)

data[['Female','Male']] = y1

#One-Hot encoder untuk data Partner
enc_oht_part = OneHotEncoder(categories=[['Yes','No']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['Partner']).reshape(-1,1)
enc_oht_part.fit(y)

y1 =  enc_oht_part.transform(y)

data[['Partner_Yes','Partner_No']] = y1

#One-Hot encoder untuk data Dependents
enc_oht_depend = OneHotEncoder(categories=[['No','Yes']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['Dependents']).reshape(-1,1)
enc_oht_depend.fit(y)

y1 =  enc_oht_depend.transform(y)

data[['Dependents_No','Dependents_Yes']] = y1

#One-Hot encoder untuk data PhoneService
enc_oht_PhoneS = OneHotEncoder(categories=[['No','Yes']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['PhoneService']).reshape(-1,1)
enc_oht_PhoneS.fit(y)

y1 =  enc_oht_PhoneS.transform(y)

data[['PhoneService_No','PhoneService_Yes']] = y1

#One-Hot encoder untuk data MultipleLines
enc_oht_Multiple = OneHotEncoder(categories=[['No','Yes']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['MultipleLines']).reshape(-1,1)
enc_oht_Multiple.fit(y)

y1 =  enc_oht_Multiple.transform(y)

data[['MultipleLines_No','MultipleLines_Yes']] = y1

#One-Hot encoder untuk data InternetService
enc_oht_InternetS = OneHotEncoder(categories=[['DSL','Fiber optic','No']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['InternetService']).reshape(-1,1)
enc_oht_InternetS.fit(y)

y1 =  enc_oht_InternetS.transform(y)

data[['InternetService_DSL','InternetService_FiberOp','InternetService_No']] = y1

#One-Hot encoder untuk data OnlineSecurity
enc_oht_OnSecure = OneHotEncoder(categories=[['No','Yes']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['OnlineSecurity']).reshape(-1,1)
enc_oht_OnSecure.fit(y)

y1 =  enc_oht_OnSecure.transform(y)

data[['OnlineS_No','OnlineS_Yes']] = y1

#One-Hot encoder untuk data OnlineBackup
enc_oht_Backup = OneHotEncoder(categories=[['Yes','No']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['OnlineBackup']).reshape(-1,1)
enc_oht_Backup.fit(y)

y1 =  enc_oht_Backup.transform(y)

data[['Backup_Yes','Backup_No']] = y1

#One-Hot encoder untuk data DeviceProtection
enc_oht_Protection = OneHotEncoder(categories=[['No','Yes']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['DeviceProtection']).reshape(-1,1)
enc_oht_Protection.fit(y)

y1 =  enc_oht_Protection.transform(y)

data[['Protection_No','Protection_Yes']] = y1

#One-Hot encoder untuk data TechSupport
enc_oht_TSupport = OneHotEncoder(categories=[['No','Yes']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['TechSupport']).reshape(-1,1)
enc_oht_TSupport.fit(y)

y1 =  enc_oht_TSupport.transform(y)

data[['TechSupport_No','TechSupport_Yes']] = y1

#One-Hot encoder untuk data StreamingTV
enc_oht_StreamTV = OneHotEncoder(categories=[['No','Yes']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['StreamingTV']).reshape(-1,1)
enc_oht_StreamTV.fit(y)

y1 =  enc_oht_StreamTV.transform(y)

data[['StreamTV_No','StreamTV_Yes']] = y1

#One-Hot encoder untuk data StreamingMovies
enc_oht_StreamMovie = OneHotEncoder(categories=[['No','Yes']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['StreamingMovies']).reshape(-1,1)
enc_oht_StreamMovie.fit(y)

y1 =  enc_oht_StreamMovie.transform(y)

data[['StreamMovie_No','StreamMovie_Yes']] = y1

#One-Hot encoder untuk data ordinal yaitu Contract
enc_oht_Contract = OneHotEncoder(categories=[['Month-to-month','One year','Two year']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['Contract']).reshape(-1,1)
enc_oht_Contract.fit(y)

y1 =  enc_oht_Contract.transform(y)

data[['Cont_Month-to-month','Cont_One year','Cont_Two year']] = y1

#One-Hot encoder untuk data PaperlessBilling
enc_oht_Billing = OneHotEncoder(categories=[['Yes','No']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['PaperlessBilling']).reshape(-1,1)
enc_oht_Billing.fit(y)

y1 =  enc_oht_Billing.transform(y)

data[['Billing_Yes','Billing_No']] = y1

#One-Hot encoder untuk data PaymentMethod
enc_oht_Payment = OneHotEncoder(categories=[['Electronic check','Mailed check','Bank transfer (automatic)',
                                             'Credit card (automatic)']] ,handle_unknown='ignore', sparse = False)

y = np.array(data['PaymentMethod']).reshape(-1,1)
enc_oht_Payment.fit(y)

y1 =  enc_oht_Payment.transform(y)

data[['Electronic check','Mailed check','Bank transfer (automatic)','Credit card (automatic)']] = y1

data.columns

data_fin = data[['SeniorCitizen','tenure','MonthlyCharges','TotalCharges', 'Female', 'Male',
       'Partner_Yes', 'Partner_No', 'Dependents_No', 'Dependents_Yes',
       'PhoneService_No', 'PhoneService_Yes', 'MultipleLines_No',
       'MultipleLines_Yes', 'InternetService_DSL', 'InternetService_FiberOp',
       'InternetService_No', 'OnlineS_No', 'OnlineS_Yes', 'Backup_Yes',
       'Backup_No', 'Protection_No', 'Protection_Yes', 'TechSupport_No',
       'TechSupport_Yes', 'StreamTV_No', 'StreamTV_Yes', 'StreamMovie_No',
       'StreamMovie_Yes', 'Cont_Month-to-month', 'Cont_One year',
       'Cont_Two year', 'Billing_Yes', 'Billing_No', 'Electronic check',
       'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)','Churn']].copy()

#pisahkan x dan y
data_x = data_fin[['SeniorCitizen','tenure','MonthlyCharges','TotalCharges', 
                   'Female', 'Male','Partner_Yes', 'Partner_No', 'Dependents_No', 'Dependents_Yes',
                   'PhoneService_No', 'PhoneService_Yes', 'MultipleLines_No',
                   'MultipleLines_Yes', 'InternetService_DSL', 'InternetService_FiberOp',
                   'InternetService_No', 'OnlineS_No', 'OnlineS_Yes', 'Backup_Yes',
                   'Backup_No', 'Protection_No', 'Protection_Yes', 'TechSupport_No',
                   'TechSupport_Yes', 'StreamTV_No', 'StreamTV_Yes', 'StreamMovie_No',
                   'StreamMovie_Yes', 'Cont_Month-to-month', 'Cont_One year',
                   'Cont_Two year', 'Billing_Yes', 'Billing_No', 'Electronic check',
                   'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)']].copy()
data_y = data_fin['Churn']

"""#Modelling to get benchmarks"""

X_train, X_test, y_train, y_test = train_test_split(data_x,data_y,test_size = 0.20, random_state = 5)

classifiers = [['DecisionTree :',DecisionTreeClassifier()],
               ['RandomForest :',RandomForestClassifier()],
               ['KNeighbours :', KNeighborsClassifier()],
               ['SVM :', SVC()],
               ['Neural Network :', MLPClassifier()],
               ['LogisticRegression :', LogisticRegression()],
               ['GradientBoostingClassifier: ', GradientBoostingClassifier()],
               ['XGB :', XGBClassifier()]]

predictions_df = pd.DataFrame()
predictions_df['actual_labels'] = y_test

for name,classifier in classifiers:
    classifier = classifier
    classifier.fit(X_train, y_train)
    predictions = classifier.predict(X_test)
    predictions_df[name.strip("")] = predictions
    scores = accuracy_score(predictions, y_test)
    print(name,'confusion_matrix : \n',confusion_matrix(y_test, predictions))
    print('accuracy scores :', scores)
    print("recall score :",recall_score(y_test, predictions))
    print("precision score:",precision_score(y_test, predictions))
    print('====================================================================')

"""Berdasarkan hasil Confusion matrix, mean of 10-fold validation, recall & preciion score, maka metode yang akan digunakan adalah :


1.   Random Forest
2.   Gradient Boosting Classifier

#MODELLING 
with balancing, tunning and Dimensionality Reduction

##Random Forest

###Imbalance Data - RForest
"""

#Balancing data
sm = SMOTE(random_state=5, sampling_strategy=0.7)
X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(data_x, data_y, test_size=0.2, random_state=5)

# Fit the over sampling
X_train_smote, y_train_smote = sm.fit_resample(X_train_smote, y_train_smote)
print("Distribution of over sampling: {}".format(Counter(y_train_smote)))
print("")

RF = RandomForestClassifier()
RF.fit(X_train_smote, y_train_smote)

#evaluasi RF
predictionRF = RF.predict(X_test_smote)
print('NIlai Akurasi:', accuracy_score(predictionRF, y_test_smote))

"""Setelah dilakukannya Balancing data dengan metode Random Forest Skor akurasi meningkat dari 76,61% menjadi **77,68%**

###Hyperparameter Tunning - RForest
"""

#define patameter grid
n_tree_RF = list([5,7,9,11]) 
max_d_RF = list([3,4,5,6]) 
crit_RF = list(['entropy','gini'])

param_grid_RF = dict(n_estimators = n_tree_RF, max_depth= max_d_RF, criterion = crit_RF) 
print(param_grid_RF)

# instantiate the grid
gridRF = GridSearchCV(RF, param_grid_RF, scoring='accuracy', return_train_score=False) 

# fit the grid with data
gridRF.fit(X_train_smote, y_train_smote)

grid_mean_scores_RF = gridRF.cv_results_['mean_test_score']
print(grid_mean_scores_RF)
print(" ")

# Show only the best model
print("Akurasi tertngginya sebesar :",gridRF.best_score_)
print(gridRF.best_params_)
print(gridRF.best_estimator_)

"""Dengan melakukan Tunning terhadap Parameternya maka Nilai akurasi mengalami peningkatan menjadi **82,03%**

###Filter Method - RForest
"""

#Dimensionality Reduction with Filter Method
filterRF = SelectKBest(f_classif, k=10) 
filterRF.fit(X_train_smote, y_train_smote)

X_train_filter_RF = filterRF.transform(X_train_smote)
X_test_filter_RF = filterRF.transform(X_test_smote)

print("Before feature selection", X_train.shape)
print("After feature selection", X_train_filter_RF.shape)

print("Score of features", filterRF.scores_)

feature_importance = pd.Series(filterRF.scores_, index=X_train_smote.columns)
feature_importance.sort_values().plot(kind='barh',figsize=(14,8))
plt.show()

# modelling with random forest
RF = RandomForestClassifier(criterion = "gini",
                            max_depth=6,
                            n_estimators=7)

RF.fit(X_train_filter_RF, y_train_smote)

# Evaluation
y_predict_train = RF.predict(X_train_filter_RF)
y_predict_test = RF.predict(X_test_filter_RF)

training_acc_RF = accuracy_score(y_train_smote, y_predict_train)
testing_acc_RF = accuracy_score(y_test_smote, y_predict_test)

print("Training Accuracy: {}".format(training_acc_RF))
print("Testing Accuracy: {}".format(testing_acc_RF))
print("recall score :",recall_score(y_test_smote, y_predict_test))
print("precision score:",precision_score(y_test_smote, y_predict_test))

"""Terjadi peningkatan Akurasi skor kembali dengan melakukan Feature Extraction meskipun tidak terlalu significant yaitu menjadi **82,73 %**"""

#Show Confusion Matrix
cm = metrics.confusion_matrix(y_test_smote, y_predict_test)

ax= plt.subplot()
sns.heatmap(cm, annot=True, fmt='', ax=ax, cmap='Blues')
ax.set_xlabel('Actual');ax.set_ylabel('Predicted')
ax.set_title('Confusion Matrix')
plt.show()

"""Berdasarkan hasil prediksi terlihat bahwasannya model mampu memprediksi data dengan menghasilkan akurasi sebesar **82,73 %** dengan detail:
* Prediksi Churn yang sebenernya benar churn adalah **273**
* Prediksi tidak Churn yang sebenernya tidak Churn adalah **814**
* Prediksi tidak Churn yang sebenernya benar Churn adalah **185**
* Prediksi Churn yang sebenernya tidak Churn adalah **135**

##Gradient Boosting

###Imbalance Data - GBoosting
"""

#Balancing data
sm = SMOTE(random_state=5, sampling_strategy=0.5)
X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(data_x, data_y, test_size=0.2, random_state=5)

# Fit the over sampling
X_train_smote, y_train_smote = sm.fit_resample(X_train_smote, y_train_smote)
print("Distribution of over sampling: {}".format(Counter(y_train_smote)))
print("")

GB = GradientBoostingClassifier()
GB.fit(X_train_smote, y_train_smote)

#evaluasi RF
predictionRF = GB.predict(X_test_smote)
print('NIlai Akurasi:', accuracy_score(predictionRF, y_test_smote))

"""Setelah dilakukannya Balancing data dengan metode Gradient Boosting Skor akurasi mengalami peningkatan yang tidak terlalu significant  yaitu sebesar 0.07% menjadi **79,31%**

###Hyperparameter Tunning - GBoosting
"""

#define ruang pencarian
lr_rate_GB = list([0.5, 0.75, 1])
n_estimator= list([110,120,130,140])
max_d_GB = list([3,4,5])
crit_GB = list(['friedman_mse','gini'])

param_grid_GB = dict(learning_rate=lr_rate_GB, n_estimators = n_estimator, max_depth= max_d_GB, criterion = crit_GB)  #Paramater untuk grid nya, nama parameter sesuai dengan hyperparameter di Random Foest
print(param_grid_GB)

# instantiate the grid
gridGB = GridSearchCV(GB, param_grid_GB, scoring='accuracy', return_train_score=False) 

# fit the grid with data
gridGB.fit(X_train_smote, y_train_smote)

grid_mean_scores_GB = gridGB.cv_results_['mean_test_score']

# Show only the best model
print("Akurasi tertngginya sebesar :",gridGB.best_score_)
print(gridGB.best_params_)
print(gridGB.best_estimator_)

"""Dengan melakukan Tunning terhadap Parameternya maka Nilai akurasi mengalami peningkatan menjadi **80,51%**

###Filter Method - GBoosting
"""

#Dimensionality Reduction with Filter Method
filterGB = SelectKBest(f_classif, k=10) 
filterGB.fit(X_train_smote, y_train_smote)

X_train_filter_GB = filterGB.transform(X_train_smote)
X_test_filter_GB = filterGB.transform(X_test_smote)

print("Before feature selection", X_train.shape)
print("After feature selection", X_train_filter_GB.shape)

print("Score of features", filterGB.scores_)

feature_importance = pd.Series(filterGB.scores_, index=X_train_smote.columns)
feature_importance.sort_values().plot(kind='barh', figsize=(14,8))
plt.show()

# modelling with Gradient Boosting
GB = GradientBoostingClassifier(criterion = "friedman_mse",
                                learning_rate=0.5,
                                max_depth=3,
                                n_estimators=120)

GB.fit(X_train_filter_GB, y_train_smote)

# Evaluation
y_predict_train_GB = GB.predict(X_train_filter_GB)
y_predict_test_GB = GB.predict(X_test_filter_GB)

training_acc_GB = accuracy_score(y_train_smote, y_predict_train_GB)
testing_acc_GB = accuracy_score(y_test_smote, y_predict_test_GB)

print("Training Accuracy: {}".format(training_acc_GB))
print("Testing Accuracy: {}".format(testing_acc_GB))
print("recall score :",recall_score(y_test_smote, y_predict_test_GB))
print("precision score:",precision_score(y_test_smote, y_predict_test_GB))

"""Terjadi peningkatan Akurasi skor kembali dengan melakukan Feature Extraction meskipun tidak terlalu significant yaitu menjadi **83,26 %**"""

#Show Confusion Matrix
cm = metrics.confusion_matrix(y_test_smote, y_predict_test_GB)

ax= plt.subplot()
sns.heatmap(cm, annot=True, fmt='', ax=ax, cmap='Reds')
ax.set_xlabel('Actual');ax.set_ylabel('Predicted')
ax.set_title('Confusion Matrix')
plt.show()

"""Berdasarkan hasil prediksi dengan metode Gradient Boosting terlihat bahwasannya model mampu memprediksi data dengan menghasilkan akurasi sebesar **83,26 %** dengan detail:
* Prediksi Churn yang sebenernya benar churn adalah **218**
* Prediksi tidak Churn yang sebenernya tidak Churn adalah **872**
* Prediksi tidak Churn yang sebenernya benar Churn adalah **127**
* Prediksi Churn yang sebenernya tidak Churn adalah **190**

###Wrapper Method - Gboosting
"""

wrapperGB = RFE(GB, n_features_to_select=10)
wrapperGB.fit(X_train_smote, y_train_smote)

X_train_wrapper_GB = wrapperGB.transform(X_train_smote)
X_test_wrapper_GB = wrapperGB.transform(X_test_smote)

print("Before feature selection", X_train.shape)
print("After feature selection", X_train_wrapper_GB.shape)

print("Score of features", wrapperGB.ranking_)

feature_importance = pd.Series(wrapperGB.ranking_, index=X_train_smote.columns)
feature_importance.sort_values().plot(kind='barh', figsize=(14,8))
plt.show()

# modelling with Random Forest
GB = GradientBoostingClassifier(criterion = "friedman_mse",
                                learning_rate=0.5,
                                max_depth=3,
                                n_estimators=120)

GB.fit(X_train_wrapper_GB, y_train_smote)

# Evaluation
y_predict_train = GB.predict(X_train_wrapper_GB)
y_predict_test = GB.predict(X_test_wrapper_GB)

training_acc = accuracy_score(y_train_smote, y_predict_train)
testing_acc = accuracy_score(y_test_smote, y_predict_test)

print("Training Accuracy: {}".format(training_acc))
print("Testing Accuracy: {}".format(testing_acc))
print("recall score :",recall_score(y_test_smote, y_predict_test))
print("precision score:",precision_score(y_test_smote, y_predict_test))

#Show Confusion Matrix
cm = metrics.confusion_matrix(y_test_smote, y_predict_test)

ax= plt.subplot()
sns.heatmap(cm, annot=True, fmt='', ax=ax, cmap='Reds')
ax.set_xlabel('Actual');ax.set_ylabel('Predicted')
ax.set_title('Confusion Matrix')
plt.show()

"""Berdasarkan hasil prediksi dengan metode Gradient Boosting disertai dengan balancing, tunning, dan feature importance terlihat bahwasannya model mampu memprediksi data dengan menghasilkan akurasi sebesar **89,48 %** dengan detail:
* Prediksi Churn yang sebenernya benar churn adalah **221**
* Prediksi tidak Churn yang sebenernya tidak Churn adalah **871**
* Prediksi tidak Churn yang sebenernya benar Churn adalah **128**
* Prediksi Churn yang sebenernya tidak Churn adalah **187**
"""

